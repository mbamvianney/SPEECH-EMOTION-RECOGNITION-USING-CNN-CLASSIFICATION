# SPEECH-EMOTION-RECOGNITION-USING-CNN-CLASSIFICATION
https://youtu.be/ApaIKjK2PhU
. This code implements a Convolutional Neural
Network (CNN) model for audio classification using the
TESS dataset. The purpose of the code is to classify audio
samples into six different emotions: angry, disgust, fear,
happy, neutral, and sad. The methodology involves
preprocessing the audio data by extracting Mel-frequency
Cepstral Coefficients (MFCCs) as features and augmenting
the dataset with various transformations. The CNN model
architecture consists of a convolutional layer, a dense layer,
and an output layer with softmax activation. The model is
trained and evaluated on the TESS dataset, achieving a test
accuracy of 94.79%. The key findings of this study indicate
that a CNN model can effectively classify emotions from
audio samples.The use of MFCC features and data
augmentation techniques such as noise addition, time
stretching, and pitch shifting improves the model's
performance. The achieved accuracy of 94.79%
demonstrates the effectiveness of the proposed approach for
audio emotion classification. These findings have
implications for applications such as emotion recognition in
speech analysis, human-computer interaction, and affective
computing.
I
